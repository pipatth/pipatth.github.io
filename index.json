[{"authors":["admin"],"categories":null,"content":"I am a machine learning engineer at Data Revenue GmbH. I designed and developed end-to-end machine learning systems for medical researchers. I am also a founder of Dataginjo LLC where I worked with clients in online education and recreation industries to apply machine learning and predictive modeling in marketing and operation areas. My interests include reinforcement learning and time-series forecasting.\nBefore moving to the U.S., I worked with VNET Venture Capital, one of the leading venture capital management companies in Thailand. My focus was on identifying investment opportunities in IT, retail, healthcare, and food industries.\nI published research articles on Quantitative Finance and Review of Quantitative Finance and Accounting. But now I am focusing on artificial intelligence in marketing.\nI enjoy beer, cooking, SCUBA diving, and looking at soccer stats\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pipatth.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a machine learning engineer at Data Revenue GmbH. I designed and developed end-to-end machine learning systems for medical researchers. I am also a founder of Dataginjo LLC where I worked with clients in online education and recreation industries to apply machine learning and predictive modeling in marketing and operation areas. My interests include reinforcement learning and time-series forecasting.\nBefore moving to the U.S., I worked with VNET Venture Capital, one of the leading venture capital management companies in Thailand.","tags":null,"title":"Pat Thontirawong","type":"authors"},{"authors":[],"categories":[],"content":"In Part1, we configured OANDA v20 and downloaded data to train our agent. Today, we are going to build a custom OpenAI Gym environment.\nCheck out my full codes on GitHub.\nGym is an open-sourced tool to train your reinforcement learning agent. Think of Gym as a training ground. The agent send an action to the environment. The environment processes it and reply with the next observation and reward. This cycle goes on until the training session (episode) ends.\nsource: https://gym.openai.com/docs/\nGym provides many pre-built environments from the basic ones such as CartPole-v0 to Atari old video games. Although Gym doesn\u0026rsquo;t provide a pre-built environment for currency trading, we can get one by creating a subclass of gym.Env class.\nBut first we need to install Gym:\n$ pip install gym Before we start building an environment, let\u0026rsquo;s understand how gym.Env class works. Adam King did a very good job explaining each method here. My code is based on his. More details about Gym can be found on Gym documentation here\nBasically, there are two public methods that you need to write:\n reset(). This gets called when you want to start a new session. step(action). You call this one when your agent takes an action.  Note: There is also a render() method but we skip it for now because we don\u0026rsquo;t need to show anything on the screen.\nA simple run of CartPole-v0 looks like this:\nimport gym env = gym.make('CartPole-v0') env.reset() # reset environment to start a session for _ in range(1000): # run for 1,000 steps env.step(1) # take action 1 env.close() Easy, right? Let\u0026rsquo;s put a currency trading touch to it.\n__init__()\nWe\u0026rsquo;re going to initialize the environment with the data that we downloaded from Part1. We also need to add common parameters such as commission (good if we want to use the code in stock trading), slippage, and the maximum number of steps that a session is going to run.\nCurrency trading is a little different from stock or Bitcoin. You have a regulatory margin requirement. OANDA published margin percentage and a guide how it calculates margin closeout here. In our case, I set margin requirement to 100% (i.e. you need to have equity to cover 100% of your position)\nAn action_space has two elements. One has three choices whether we long, short, close all positions, or hold. The other is the proportion that we long or short (based on total margin available). This can range from 1/10, 2/10, to 10/10.\nTo make it easy to report portfolio performance, I define the size of account_history dataframe to 5 columns. We\u0026rsquo;ll send out net asset value nav, units long/short, and proceeds in a pandas dataframe later.\n# init def __init__( self, df, commission=0.0, margin_req=1.00, closeout_req=0.5, slippage=0.0, initial=100000, serial=False, max_steps=200, ): super(TradeEnv, self).__init__() self.lookback_sz = 24 self.commission = commission self.margin_req = margin_req self.closeout_req = closeout_req self.slippage = slippage self.initial = initial self.serial = serial self.max_steps = max_steps self.colTime = \u0026quot;time\u0026quot; self.cols = { \u0026quot;volume\u0026quot;: \u0026quot;volume\u0026quot;, \u0026quot;ask\u0026quot;: \u0026quot;ask.c\u0026quot;, \u0026quot;bid\u0026quot;: \u0026quot;bid.c\u0026quot;, \u0026quot;open\u0026quot;: \u0026quot;mid.o\u0026quot;, \u0026quot;high\u0026quot;: \u0026quot;mid.h\u0026quot;, \u0026quot;low\u0026quot;: \u0026quot;mid.l\u0026quot;, \u0026quot;close\u0026quot;: \u0026quot;mid.c\u0026quot;, } self.ac_cols = [\u0026quot;nav\u0026quot;, \u0026quot;unit_long\u0026quot;, \u0026quot;value_long\u0026quot;, \u0026quot;unit_short\u0026quot;, \u0026quot;value_short\u0026quot;] self.df = df.dropna().sort_values(self.colTime) self.action_space = spaces.MultiDiscrete([4, 10]) self.df_sz = self.df.shape[1] - 1 self.observation_space = spaces.Box( low=0, high=1, shape=(self.df_sz + len(self.ac_cols), self.lookback_sz + 1), dtype=np.float16, ) reset()\nreset() job is to set balance to the seed money that we give the agent. The units_open and prices_open lists are queues to keep track of positions that we have. OANDA requires a FIFO (first-in, first-out) so for example, if you are closing half of your current position, OANDA will close the oldest one first.\nWe call _reset_pos() private method (details here) to find the starting position of the dataframe. Remember that we have 30,000 training data points in Part1? This method randomly pick one starting point from that.\nAs I mentioned earlier, we create three pandas dataframes to help us track and report trades, prices, and the account.\nAt the end of the method, it\u0026rsquo;s a convention in Gym to get the next observation and pass it out. We call a _next_obs() method.\n# reset def reset(self): self.balance = self.initial self.units_open = [] # FIFO queue self.prices_open = [] self._reset_pos() # account history df ac_array = np.repeat([[self.initial, 0, 0, 0, 0]], self.lookback_sz + 1, axis=0) self.ac_hist = pd.DataFrame(ac_array, columns=self.ac_cols) # price history df pr_cols = [self.cols[k] for k in self.cols.keys()] self.pr_hist = self.df.loc[ self.start_pos - self.lookback_sz : self.start_pos, pr_cols ] self.pr_hist.columns = [k for k in self.cols.keys()] # trade history df tr_cols = [\u0026quot;step\u0026quot;, \u0026quot;type\u0026quot;, \u0026quot;unit\u0026quot;, \u0026quot;total\u0026quot;] self.tr_hist = pd.DataFrame(None, columns=tr_cols) return self._next_obs() _next_obs()\n_next_obs() get observation (aka information) and pass it to the agent. Our agent will use this information as an input and come up with the next action. We can get creative with technical indicators (see https://www.ta-lib.org/) but let\u0026rsquo;s keep things simple for now with OHLCV (\u0026ldquo;open\u0026rdquo;, \u0026ldquo;high\u0026rdquo;, \u0026ldquo;low\u0026rdquo;, \u0026ldquo;close\u0026rdquo;, \u0026ldquo;volume\u0026rdquo;) and account history.\nSimilar to other neural networks, most algorithms run better when we scale the data.\n# get next obs def _next_obs(self): # OHLCV info end = self.current_step + self.lookback_sz + 1 pr_hist = ( self.active_df.iloc[self.current_step : end] .drop(self.colTime, axis=1) .values ) obs = preprocessing.StandardScaler().fit_transform(pr_hist).T # append scaled history scaled_hist = preprocessing.MinMaxScaler().fit_transform(self.ac_hist.T) obs = np.append(obs, scaled_hist[:, -(self.lookback_sz + 1) :], axis=0) return obs step(action)\nThis method is called every time the agent is taking an action. After taking an action (_take_action method), we move current_step forward and inform the agent the reward and the next observation. One thing to note here is we force sell if episode ends.\n# take a step forward def step(self, action): nav_beg = self.get_nav() pr_row = self._get_pr_row() self._take_action(action, pr_row) self.remaining_steps -= 1 self.current_step += 1 # end of episode, force close if self.remaining_steps == 0: unit_to_close = -sum(self.units_open) # reverse to close if unit_to_close \u0026gt; 0: price = pr_row[\u0026quot;ask\u0026quot;] self._close_pos(unit_to_close, price) elif unit_to_close \u0026lt; 0: price = pr_row[\u0026quot;bid\u0026quot;] self._close_pos(unit_to_close, price) self._reset_pos() obs = self._next_obs() reward = self.get_nav() - nav_beg done = self._is_closeout() # append next price to pr_hist pr_cols = [self.cols[k] for k in self.cols.keys()] pr_next = self.df.loc[self.start_pos + self.current_step, pr_cols] pr_next.index = [k for k in self.cols.keys()] self.pr_hist = self.pr_hist.append(pd.DataFrame(pr_next).T) return obs, reward, done, {} _take_action(action, pr_row)\nInstead of just long/short/close/hold, we have to be careful of the imposed margin requirement. avail indicates how much margin we have available after subtracting margin used by the positions that we already opened.\nNote: I didn\u0026rsquo;t store margin used and NAV but wrote few private methods to compute them from positions opened.\nIf an action is to close all positions (action_type == 2), we don\u0026rsquo;t need to care about margin, just reverse the number of units that we have and close them using the right bid/ask. Notice that we apply slippage here.\nIf an action is either 0 or 1 which mean long or short, we check how many units we can take based on available margin. We need to round up the units to whole numbers here.\n# action def _take_action(self, action, pr_row): action_type = action[0] proportion = action[1] / 10 avail = self.get_nav() - self._get_margin_used() # margin available to use unit_filled = 0 price = 0 # close if action_type == 2: unit = -sum(self.units_open) if unit \u0026gt; 0: price = pr_row[\u0026quot;ask\u0026quot;] * (1 + self.slippage) else: price = pr_row[\u0026quot;bid\u0026quot;] * (1 - self.slippage) unit_filled = self._close_pos(unit, price) # take pos elif avail \u0026gt; 0: # long if action_type == 0: price = pr_row[\u0026quot;ask\u0026quot;] * (1 + self.slippage) unit = int(avail / price * proportion) # unit to long unit_filled = self._open_pos(unit, price) # short elif action_type == 1: price = pr_row[\u0026quot;bid\u0026quot;] * (1 - self.slippage) unit = -int(avail / price * proportion) # unit to short unit_filled = self._open_pos(unit, price) # record trade history if unit_filled != 0: tr = pd.DataFrame( [ [ self.start_pos + self.current_step, \u0026quot;long\u0026quot; if unit_filled \u0026gt; 0 else \u0026quot;short\u0026quot;, unit_filled, unit_filled * price, ] ], columns=self.tr_hist.columns, ) self.tr_hist = self.tr_hist.append(tr, ignore_index=True) # record account history if unit_filled \u0026gt; 0: unit_long = abs(unit_filled) unit_short = 0 elif unit_filled \u0026lt; 0: unit_long = 0 unit_short = abs(unit_filled) else: unit_long = 0 unit_short = 0 df_ = pd.DataFrame( [ [ self.get_nav(), unit_long, unit_long * price, unit_short, unit_short * price, ] ], columns=self.ac_hist.columns, ) self.ac_hist = pd.concat([self.ac_hist, df_], ignore_index=True) get_summary()\nLastly, we add another public method to get a summary of the account and the prices in pandas dataframe. This also make it easy to update metrics such as Sharpe or maximum drawdown later as the agent trades.\n# get summary def get_summary(self): accounts = self.ac_hist.reset_index(drop=True) prices = self.pr_hist.reset_index(drop=True) accounts[\u0026quot;gl\u0026quot;] = accounts[\u0026quot;nav\u0026quot;].diff().fillna(0) accounts[\u0026quot;ret\u0026quot;] = (accounts[\u0026quot;gl\u0026quot;] / accounts[\u0026quot;nav\u0026quot;].shift(1)).fillna(0) return pd.concat([prices, accounts], axis=1) Voila! we have a trading environment. We can start training our bot in Part3.\n","date":1565728310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565728310,"objectID":"472ebb740f3c224073188efc0ac10162","permalink":"https://pipatth.github.io/post/ppo2-currency-trader-part2/","publishdate":"2019-08-13T16:31:50-04:00","relpermalink":"/post/ppo2-currency-trader-part2/","section":"post","summary":"In Part1, we configured OANDA v20 and downloaded data to train our agent. Today, we are going to build a custom OpenAI Gym environment.\nCheck out my full codes on GitHub.\nGym is an open-sourced tool to train your reinforcement learning agent. Think of Gym as a training ground. The agent send an action to the environment. The environment processes it and reply with the next observation and reward. This cycle goes on until the training session (episode) ends.","tags":[],"title":"Building a bot to trade foreign currency using reinforcement learning (Part 2)","type":"post"},{"authors":[],"categories":[],"content":"Today we\u0026rsquo;re going to build an environment to train a reinforcement learning agent to trade foreign currency. We\u0026rsquo;ll be using OpenAI Gym as our tool. Gym custom environment is very flexible to set up and you can apply the idea into other reinforcement learning projects.\nSo here\u0026rsquo;s the list of what we\u0026rsquo;re going to do:\n Set up OANDA v20 to download data from OANDA (Part1) Write an OpenAI Gym to simulate the foreign exchange market (Part2) Train an agent to trade using Proximal Policy Optimization algorithm (Part3) See how the agent performs on Dash app (Part3)  You can clone my code on GitHub.\nSo here we go.\nLet\u0026rsquo;s begin by setting up an OANDA API so we can download historical data from OANDA. You can go to OANDA and sign up for a free FXTrade Practice account here. After you\u0026rsquo;re done, OANDA will give you token information. You\u0026rsquo;ll need to put the information in your .v20.conf file in your $HOME directory.\nSomething like this:  OANDA provides a python library called v20. With this library, we don\u0026rsquo;t have to write our own HTTP calls. Let\u0026rsquo;s clone and install that on your virualenv environment. We also gonna need pandas and PyYAML\n$ pip install v20 pandas PyYAML You can also use v20 for live trading (if your bot find golden nuggets!!!). The documentation here is quite comprehensive.\nBut for now, let\u0026rsquo;s write some python to get historical data.\nFirst, we read the config file .v20.conf from our $HOME path and create an API context. OANDA needs this to verify our accounts.\nThen, we write a function get_hourly_candle to get n data points from OANDA. I stick with hourly data, but you can change the granularity to download daily, monthly, or even 5-second data. Let\u0026rsquo;s do BAM to get \u0026lsquo;bid\u0026rsquo;, \u0026lsquo;ask\u0026rsquo;, and \u0026lsquo;mid\u0026rsquo; points.\nBecause OANDA allows maximum 5,000 bars per HTTP call, we need to write a loop to download data in chunks smaller than 5,000. The function get_data did just that. We then concatenate, reset index, and save data to a tab-delimited file.\n So if you call get_data function like below, you should get 30,000 bars of data\nget_data(\u0026quot;EUR_USD\u0026quot;, \u0026quot;2014-01-01\u0026quot;, 30000, \u0026quot;EUR_USD_train.tsv\u0026quot;) We also want to download some data to test the agent. This so-called \u0026lsquo;testing\u0026rsquo; dataset should not be the same as the one we use to train the agent\nget_data(\u0026quot;EUR_USD\u0026quot;, \u0026quot;2019-01-01\u0026quot;, 2000, \u0026quot;EUR_USD_test.tsv\u0026quot;) So now we have some data. They should look something like this:  Now we\u0026rsquo;re ready to build a trading environment. Part2 here.\nNote: I used EUR_USD and GBP_USD as examples here. It\u0026rsquo;s easier to work when you have a quote in your home currency. If your home currency is USD but you\u0026rsquo;re trying to do USD_JPY or USD_CAD, just be careful.\n","date":1565638121,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565638121,"objectID":"b5add19589280a9a5ba260e08863d00d","permalink":"https://pipatth.github.io/post/ppo2-currency-trader-part1/","publishdate":"2019-08-12T15:28:41-04:00","relpermalink":"/post/ppo2-currency-trader-part1/","section":"post","summary":"Today we\u0026rsquo;re going to build an environment to train a reinforcement learning agent to trade foreign currency. We\u0026rsquo;ll be using OpenAI Gym as our tool. Gym custom environment is very flexible to set up and you can apply the idea into other reinforcement learning projects.\nSo here\u0026rsquo;s the list of what we\u0026rsquo;re going to do:\n Set up OANDA v20 to download data from OANDA (Part1) Write an OpenAI Gym to simulate the foreign exchange market (Part2) Train an agent to trade using Proximal Policy Optimization algorithm (Part3) See how the agent performs on Dash app (Part3)  You can clone my code on GitHub.","tags":[],"title":"Building a bot to trade foreign currency using reinforcement learning (Part 1)","type":"post"}]